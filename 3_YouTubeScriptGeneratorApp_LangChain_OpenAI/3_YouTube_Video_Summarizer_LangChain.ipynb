{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 01: Install All the Required Packages**"
      ],
      "metadata": {
        "id": "WhDQokSKxkO5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOHXi5zGxcRB",
        "outputId": "367ae12d-5986-4bfe-cdb4-7ec3027c3604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.232-py3-none-any.whl (1.3 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.18)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.9-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.0.6,>=0.0.5 (from langchain)\n",
            "  Downloading langsmith-0.0.5-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.11)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, marshmallow-enum, langsmith, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.9 langchain-0.0.232 langsmith-0.0.5 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube-transcript-api\n",
        "!pip install pytube"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sb-lQjB4xu6C",
        "outputId": "1c14c779-bc8c-4898-e92c-98608784e325"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.4)\n",
            "Installing collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.6.1\n",
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 02: Import All the Required Libraries**"
      ],
      "metadata": {
        "id": "3uDtYwo2x96R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import YoutubeLoader"
      ],
      "metadata": {
        "id": "3v23HgOwx6lM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "PaIl2hD6yHGr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain"
      ],
      "metadata": {
        "id": "Bhiaqh7xyPtP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 03: Setup the Environment**"
      ],
      "metadata": {
        "id": "aTau0QAlyma1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = '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'"
      ],
      "metadata": {
        "id": "qd9WsJzQyYD8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 04: Load the Youtube Video and Generate a Transcript of it**"
      ],
      "metadata": {
        "id": "xpeLWg27yzwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = YoutubeLoader.from_youtube_url(\"https://www.youtube.com/watch?v=Xx9qXZ0hHC8\", add_video_info=True)"
      ],
      "metadata": {
        "id": "u4afmudiy5oF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = loader.load()"
      ],
      "metadata": {
        "id": "5IHZ_k6ezIQ3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QXhTYc9zNN1",
        "outputId": "8c2024df-1a04-461b-d656-eaca13a03374"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Found Video from {result[0].metadata['author']} that is {result[0].metadata['length']} seconds long\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kg_vd-XwzO_i",
        "outputId": "dbd0c0c8-1833-4867-d601-e97a18bc484f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found Video from Muhammad Moin that is 312 seconds long\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cZ5wf2izgsO",
        "outputId": "fd7cfa16-dbc0-4ae7-d281-8e84c2e2c19b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content=\"hello everyone I have an exciting news new object detection model yoloas is just released today Desi Ai and AI optimization company have launched a new object detection model known as YOLO Nas so let's see what is Viola Nas and key features of yellow Nas you don't knows modern has been developed using the neural architecture search technology which allows the creation of efficient and high performing deep learning models so there are architecture surge is an automated process that searches for the optimal neural network architecture for a particular toss it does this by exploring a vast search base of possible architectures and selecting the most efficient and high performing one so let's explore the key features of your load Nas so yolonas enhanced the detection of small objects and improves the accuracy and gives better performance by compute ratio in front of you you can see the comparison of YOLO V5 Euro V8 Euro V7 Euro Nas has been presented for small objects and we can see that Euro Nas outperforms your lobby 5 Euro V7 and Euro V8 models so let's explore some more key features of your learners the first point which we will discuss is new quantization friendly Block in no load Nas so YOLO Nas architecture contains quantization aware blocks and selective quantization for optimized performance so Unitas or performs better than previous models because you learn as features a novel basic law that is tailor made for quantization the second point which we will discuss is pre-trained on top data set so you don't knows pre-trained models comes with are trained on the top data sets which consist of dual Coco data set object 365 and roboflow 100 data sets and yolonas outperforms all other existing Euro models including your lobby 8 Euro V7 Euro V5 on roboflow 100 data set and the last point which I will discuss is post training quantization so after training the YOLO Nas model is converted into int8 quantization quantized version making it more efficient than other YOLO models according to Desi AI Dashi AI is an AI optimization company which has launched YOLO Nas so YOLO Nas is around 0.5 mean average Precision Point more accurate and 10 to 20 percent faster than equivalent variance of yellow V8 and YOLO V7 euron ask comes with six different models YOLO Nas s is the smallest but it is less accurate than other yolonas models while Yono Nas and the last in the table is the most accurate but it is less fast than other yolonas models so we can say that yolonas s is the fastest but it is less accurate than other YOLO Nas model because we can see here YOLO Nas S as a mean average Precision of 47.5 percent so but it is the most fastest as we can see from the latency 3.21 millisecond but the other Nas L in8 is the most accurate we can see that it has the mean average Precision of 52.1 percent but it is less fast as compared to other YOLO Nas models we can see that it has a latency of 4.78 millisecond so mean average Precision numbers in the table reported over here are from the Poco 2017 validation data set and latency Benchmark for 640 cross 640 images is done on Nvidia T4 GPU so here a comparative analysis of yolonas versus other YOLO model is presented we can see that yolonas model outperforms all other Euro models in terms of mean average Precision considering Robo flow 100 data set as The Benchmark so your last Model can be seen that that gives us better mean average Precision on roboflow or flow 100 data set in comparison to other YOLO models so I have tested your learners model on multiple images and videos and we can see that your NOS model outperforms all other YOLO models in terms of speed and accuracy and here you can see that the YOLO NOS model is able to detect that person cars trucks handbag tennis rackets and other things so we can simply say that YOLO Nas model outperforms all other YOLO models thank you for watching this video tutorial see you all in the next video till then bye bye\", metadata={'source': 'Xx9qXZ0hHC8', 'title': \"What's New in YOLO-NAS | Is YOLO-NAS the Future of Object Detection ?\", 'description': 'Unknown', 'view_count': 1739, 'thumbnail_url': 'https://i.ytimg.com/vi/Xx9qXZ0hHC8/hq720.jpg', 'publish_date': '2023-05-03 00:00:00', 'length': 312, 'author': 'Muhammad Moin'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm=OpenAI(temperature=0.6)"
      ],
      "metadata": {
        "id": "Ddi_u6-vzmgP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 05: YouTube Video Summary**"
      ],
      "metadata": {
        "id": "KDWVzKtY0Dg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(llm=llm, chain_type = 'stuff', verbose=True)"
      ],
      "metadata": {
        "id": "TgHutmVAzxSE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "dpyuXErp0SvZ",
        "outputId": "fa7c6f35-ff19-45ca-d471-b924d2279d39"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"hello everyone I have an exciting news new object detection model yoloas is just released today Desi Ai and AI optimization company have launched a new object detection model known as YOLO Nas so let's see what is Viola Nas and key features of yellow Nas you don't knows modern has been developed using the neural architecture search technology which allows the creation of efficient and high performing deep learning models so there are architecture surge is an automated process that searches for the optimal neural network architecture for a particular toss it does this by exploring a vast search base of possible architectures and selecting the most efficient and high performing one so let's explore the key features of your load Nas so yolonas enhanced the detection of small objects and improves the accuracy and gives better performance by compute ratio in front of you you can see the comparison of YOLO V5 Euro V8 Euro V7 Euro Nas has been presented for small objects and we can see that Euro Nas outperforms your lobby 5 Euro V7 and Euro V8 models so let's explore some more key features of your learners the first point which we will discuss is new quantization friendly Block in no load Nas so YOLO Nas architecture contains quantization aware blocks and selective quantization for optimized performance so Unitas or performs better than previous models because you learn as features a novel basic law that is tailor made for quantization the second point which we will discuss is pre-trained on top data set so you don't knows pre-trained models comes with are trained on the top data sets which consist of dual Coco data set object 365 and roboflow 100 data sets and yolonas outperforms all other existing Euro models including your lobby 8 Euro V7 Euro V5 on roboflow 100 data set and the last point which I will discuss is post training quantization so after training the YOLO Nas model is converted into int8 quantization quantized version making it more efficient than other YOLO models according to Desi AI Dashi AI is an AI optimization company which has launched YOLO Nas so YOLO Nas is around 0.5 mean average Precision Point more accurate and 10 to 20 percent faster than equivalent variance of yellow V8 and YOLO V7 euron ask comes with six different models YOLO Nas s is the smallest but it is less accurate than other yolonas models while Yono Nas and the last in the table is the most accurate but it is less fast than other yolonas models so we can say that yolonas s is the fastest but it is less accurate than other YOLO Nas model because we can see here YOLO Nas S as a mean average Precision of 47.5 percent so but it is the most fastest as we can see from the latency 3.21 millisecond but the other Nas L in8 is the most accurate we can see that it has the mean average Precision of 52.1 percent but it is less fast as compared to other YOLO Nas models we can see that it has a latency of 4.78 millisecond so mean average Precision numbers in the table reported over here are from the Poco 2017 validation data set and latency Benchmark for 640 cross 640 images is done on Nvidia T4 GPU so here a comparative analysis of yolonas versus other YOLO model is presented we can see that yolonas model outperforms all other Euro models in terms of mean average Precision considering Robo flow 100 data set as The Benchmark so your last Model can be seen that that gives us better mean average Precision on roboflow or flow 100 data set in comparison to other YOLO models so I have tested your learners model on multiple images and videos and we can see that your NOS model outperforms all other YOLO models in terms of speed and accuracy and here you can see that the YOLO NOS model is able to detect that person cars trucks handbag tennis rackets and other things so we can simply say that YOLO Nas model outperforms all other YOLO models thank you for watching this video tutorial see you all in the next video till then bye bye\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Desi AI, an AI optimization company, has released a new object detection model called YOLO Nas. YOLO Nas uses neural architecture search technology to create efficient and high-performing deep learning models. YOLO Nas has improved detection of small objects and outperforms other YOLO models in terms of speed and accuracy. YOLO Nas has been tested on multiple images and videos and is able to detect people, cars, trucks, handbags, and other objects.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 06: Load a Very Long Youtube Video and Generate a Transcript for it**"
      ],
      "metadata": {
        "id": "n5-xnW2P0nAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = YoutubeLoader.from_youtube_url(\"https://www.youtube.com/watch?v=aJHBDCFAZ-g\", add_video_info=True)"
      ],
      "metadata": {
        "id": "Wj5jM3BP0Vdb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=loader.load()"
      ],
      "metadata": {
        "id": "xycCtuAy0w8d"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOP55y2s0zAd",
        "outputId": "822c5f23-5486-48fd-cc99-8af8527219cc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Found Video from {result[0].metadata['author']} that is {result[0].metadata['length']} seconds long\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InpUJQW202Dp",
        "outputId": "83fd2579-d825-4ee6-ce36-6478ddce3278"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found Video from Muhammad Moin that is 2367 seconds long\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8PypQmv04Pq",
        "outputId": "dae9f377-b0f5-47e9-b9b9-98df2702ac45"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content=\"hello everyone in this video tutorial we will see how we can do license plate detection and recognition using YOLO Nas so here is the complete notebook script I will be guiding you step by step on how you can do license plate detection and recognition using yolanas for license plate detection I will be using your own asmo object detection order and to read the text from the lessons plate I will be using easy OCR so to detect a license plate I will use YOLO Nas model and to read the text on the lessons plate I will be using easy OCR so let's get started so here you can see that we have the license plate data set over here the data set consists of 300 images and here we have the air ratio like we have 601 images in the training set 64 images in the validation set and 30 images in the test set and the splitting ratio is eighty percent six percent of the images are in a training set nine percent of the images are in the validation set and four percent of the images are in that testing set and if you just see over here we have only one class in this data set which is of lessons and here you can see the images so you can see that we have only one class in our data set which is of lessons we don't have multi-classes and here you can see that a bounding box uh is being created around the lessons plate in the images like you can see over here uh bounding boxes are created around the lessons plate in the images like you can see over here you can see over here as well and you can see over here as well like I like bounding boxes have been created around the license plates if I go to the next image you can see that we have the bounding box around the lessons plate so I will be training or fine tuning my YOLO YOLO Nas model on this license plate data set so after training the Uranus model on this lessons play data set we will be testing our model on images and videos and we will be seeing whether our model is able to detect a license plate or not so if our model is able to detect the license plate then we will read the text on the lessons plate using easy or CR okay so you can see over here I will just use easy or CR to read the text on the lessons plate like here we have mh20bq20 so first of all I will be training or fine tuning the yolona small model on this lessons play data set so that we can detect the lesson split and after detecting that license plate I will use Easy OCR to read the text on the lessons plate okay so here is the code so before running the script uh please make sure that you have selected the GPU so I will just uh click on runtime and click on change runtime and you can see over here I have selected the hardware accelerator as GPU and GPU type is T4 okay so in the first step you need to install the packages so I have already run this cell and installed the packages because it's a Time ticking and take around 10 to 11 minutes to install the packages so after installing the packages we need to restart the runtime so I will just click on hit restart runtime over here and just restart the runs time then in the next step I will import all the required libraries so I will just run this cell and this cell and this cell as well so now uh after importing all the required libraries in the steps three I need to set the checkpoint directory and just write the and create a subfolder inside the checkpoint directory where I will just save my best model weights the model based on the last EPO and the average model weights plus I will also save the log files in this folder as well so now you can see here uh I will be creating a directory by the name checkpoints and inside this directory I will be creating a folder by the name anpr YOLO Nas run this is the name of the folder which I will be creating in this directory and inside this folder I will be saving the best model weights the best model which means the the the model weights uh one uh so on the specific on that specific Epoch on which the model per gives the best performance or the best mean average Precision so we call this uh this uh these weights as the best model weight so the best model weights are those weights on which the model give us the best performance okay so if I am training my model on 15 number of epoch so on the specific Epoch one which the model gives us me the best performance uh I will save those weights and call those weights as the best model weights okay and then we have the last Model bit so the in that folder so the last Model we should present the model weights on the last Epoch so I find training by yolonas model on a sense play data set for 15 number of epochs so the last model which represents the model based on the 15th Epoch because I am just training my models on 15 number of epochs so the last Model weights represent the model based on the last Epoch and then we have average modern weights and plus we also have a logs file over here in this directory as well so I'm just instantiating the trainer over here so I will just download this data set from roboflow into the Google app notebook so I will just click on overview from here and click on download this data set and just select the yellow v5pi touch format and click on continue and okay so now I will just copy this code from here and just paste this code into my Google collab notebook okay so I'm just uh okay just remove this it's double copied so I have just copied the code from there and you can see that I've just added the code over here so if you haven't created your account on roboflow you can just create your account on roboflow and this data set is publicly available on roboflow universe and you can easily download this data set from roboflow into your Google collab notebook so now you can see over here I have downloaded the data set from roboflow into the Google app notebook uh okay so here is the data set okay we have the train test validation folder and the test folder contains the images and their labels in the label we have labels folder we have the coordinates for the bounding box for each of the image and here is the data dot yml file uh you can see here we have only one class which is of lessons and here we have the training validation and test data set images path so in the images folder we have the images without bounding boxes in the labels folder we have the bonding box coordinates saved in the dot txt file okay for each of the image so I'm just setting this data set as my current directory by just click on copy path from here and set this data set as your current directory and then along with this we will set the train images and labels directory over here and then we'll also set the valid images and label structure over here and then I will also set the test images and labels directory over here as well and as you can see in the data.yml file we have only one class which is of lessons so I have just written lessons over here and then I'm just passing the batch size value number of workers value in the data set parameters and I'm just running this cell over here I am no I'm just as inspecting the data set which I have defined earlier so now I will just uh plot the images in that my training data with their bounding boxes to see how my data set images look like so I'm just plotting uh the images in the data set with their bounding boxes to see how my training data images look like you can see over here we have on the bounding box around the lessons plate so you can see over here we have drawn bounding boxes around the lessons players you can see over here here we have drawn the bounding boxes around the license plate so now I'm just instantiating the model so now I will be training or fine tuning my YOLO na small model on the lessons plate data set so I will be training or fine tuning my Yolanda small model on the lessons play data set yolonas comes with three different models Yolanda small Uranus medium and yulonas large in this project I am using you learn a small model so I'm just instantiating the Yolanda small model over here and I'm just using the pre-trained weights of the Poco data set and the number of classes uh here you can see over here if I just go above in the data set parameters I have just defined a classes and I have only one class which is of lessons so the length of this at the length of this data set parameter classes will be one because we have one class in our data set okay so now before we start the training there are a few migratory arguments that you must Define before we start the training which include maximum number of epochs so in the maximum number of epochs you will Define the number of epochs on which you want to train or fine tune the yolonas model on your custom data set currently in this case we have our custom data set as license plate data set so here we Define the maximum number of epochs then we Define the loss whether we are using PPU loss or any other loss then we Define the optimizer whether we want to use Adam Optimizer or RMS prop and then we Define the train Matrix Place validation Matrix list and then we also Define the metrics to watch so here the metrics to watch will be mean average Precision with an IOU of 0.5 so now you can see over here these are the mandatory arguments that you must Define before training uh you like here you can see that I have to find out number of parameters before training but these are the six mandatory arguments that you must Define for the training you can see over here I have defined the maximum number of epochs as 15 I have defined a loss spp although e-loss and further I have to find the optimizer as or Adam and I have defined the initial Landing rate over here as well and the Matrix to the watch I have defined is as mean average percent with an IU of 0.5 foreign videos into my drive so I'm just downloading the demo videos from drive into this Google collab notebook so I will be testing my train model on lessons play data set on this videos and see whether we are able to detect the license plate in these videos or not so let's see whether we are able to detect the lessons players on these videos or not so now here you can see that we have the demo videos over here so I have already trained my model on the license bail data set so I will not start the training again because training takes 30 to 40 minutes so I will just save this time I have already trained the more models on license plate data set and I have saved my model check best weights into the drive so I've just download the best ways from the drive into this Google collab notebook so I'm just downloading the best ways from the drive into the Google app notebook so you can see over here I stopped downloading the best ways from the drive into this Google collab notebook so I've downloaded the best ways from the drive into the Google collab notebook so I have trained my YOLO Nas or fine tune the yolona small model on the lessons play data set and here I've just defined my Pathways checkpoints path and you can see over here here is our best bits path I will just copy this path from here and just add this path over here okay so I'm now I will evaluate the best train model on the test data set images and see what we uh what we get in terms of mean average precision and in terms of F1 score recall score so let's see this might take few seconds so after this I will be doing prediction on an image as well so now you can see over here uh we have got an mean average procedure of 90.88 so this is very impressive the mean of recipient with IOU of 0.5 is 0.9088 which is 19.90.88 and we have F1 score with IOU 0.5 as 0.1017 so I will now just click on validation and I will just take a random image from here copy path and just add this image over here and let's see whether we are able to take the license plate in this image or not so let's see whether we are able to check the license plate in this sample image or not okay it is given me that that button okay so let's see uh if it gives the error or its works fine okay so just let me just copy this again I might have missed something so let's see how it goes works so now you can see that we are able to detect the license plate in this sample image and here we have the confidence of 0.79 and we have detected the sense plate let us test one other image let's copy path from here I am text and I am testing on this second image and let's see you can see over here we are able to click the license plate over here as well so the results are fine let me test on some other image so we are able to detect the lessons plate and it's working very profine till now you can see this is wrong in this case Let's test on some other image and let's see whether we are able to take the lessons play it or not you can see over here in this case we are also able to take the license plate although there is a car crash so now I will be testing my model on this video and let's see uh what results do we get so and now we will doing the direction on a video and let's see what results we are getting over here so the processing detection is done frame by frame so this will take some time because the detections will be done by frame by frame so this will be a bit time taking process and let's see whether we are able to take the license plate here or not on this video okay so now you can see here we have reductions.emp4 so here we have saved the output video by the name detections.mp4 so let me just display this output video over here okay so just remove this currently we don't need this so let's see whether we are able to take the license plate in this video or not and so I have just saved the output video by the name basically I've done the directions over here you can just remove this we don't need it so you can see over here I have done detection on this video over here now I'm just displaying the output video into the our in this Google collab notebook so I'm just displaying the output video over here in this Google app notebook so this might take few more seconds before we have the output video ready and then I will show you the output video over here so here is our output video uh due to small screen size over here we might not be able to see the output very clearly let me download this output video and share you uh share you the however detection results look like so let me just navigate my screen towards this output video so now you can see over here we are able to take the lesson step very clearly like detection results are very impressive we are able to click the lessons plate and the detection results look super awesome okay so now you can see that we are able to detect the license plate in this video we have also detected a license plate in random images as well so now uh you can see that we have directed a license plate now I will introduce easy or CR to read the text on this lessons plate okay so now in the next part we will see how we can use easy or CR to read the text on these lessons plates okay so we have reacted a license plate using yolonas object detection model so we have detected a sense plate using Uranus object detection model so now I will use Easy OCR to read the text on these lessons plate like here it is 66 hs07 foreign model now in this part we will see how we can use Easy OCR to read the text on the license plate so we have deducted the lessons plate using yodanas since now we will read that read the text on the sense plate using easy or CR so first of all you can see over here I have created a folder by the name automatic number update recognition now I will just go to the pycharm so here you can see that I am in the pycharm I will click on file and create a new project over here and here from here I will just select the directory of the upper Jack so inside the urana scores I have clicked the folder by the name automatic number plate recognition so here our also location is also fine we have creating a new virtual environment and our base interpreter is being set as python 3.10 with python exe so now I am just creating a new project over here so first of all uh we have detective license plate now we want to read the text on the lessons plate so I have saved my best model weight so I will just create a new folder over here and just add the model weights over here so now new and just create a folder by the name Waits now I will add the model weights into this folder okay let me navigate my screen okay so now you can see over here I have just created a folder by the name weights over here and inside this folder I've added my best model weights over here okay so these are the model weights after training I have just downloaded those weights and just paste into this local directory the best model weights okay so now I will just go back towards the code so now I will be doing lesson split detection on some video because it cannot be done currently in live webcam feed I'm just sitting in my work room so I will just create a new directory over here by the name video and inside this video directory I will paste our demo video so I will let me just go to the video directory so you can see over here uh inside this we have I've just created a directory by the name video and inside this directory I will just add this demo video so we will be doing license plate Direction on this video let me just show you uh what this video is okay so sorry okay so let me just navigate my screen towards this video so now you can see this video so now I will be doing lessons play detection on this video so you can see over here we will be detecting the license plate using Euro Nas and using Eco easy OCR I will read a text on these lessons plates so let me just go back towards the code so uh now here you can see that in the video folder we have the demo video in the bits folder we have the best model weights so we are on the track so in the next step what I will do over here is I will just create a DOT by file okay so here I will just create a and PR automatic number plate recognition anpr dot Pi file so now in this anpr dot Pi file I will add some code of so I will add the code for the of lesson spray detection using YOLO Nas okay so now here you can see that in this anpr dot Pi file I have added a code for the license plate detection and we have only one class license and we have a fine tune a train our yolona small model one lesson spray data set and these are the our model weights so we have placed the model weights into the weights folder over here and so just correcting this path we have to go to the bits folder and inside the bits folder we have our best model weights okay so now uh let's run this script and see uh what results do we get so I will just write python a n p r dot pi and let's do a sense play detection on this video so we are just testing for the license plate detection after this I will add code uh for the easy OCR so that we can read the text on the license plate but firstly I'm seeing whether we are able to do detect the lessons plate over here in this python or not so if there is any error I will definitely fix it but let's see how it goes so it has started execution okay so currently I'm just getting the error what is the error and the opencv python okay it should be output dot Avi okay out dot right frame okay just skip this out currently ah let me just run this now and let's see how it works and if there is any error we will fix it out so it has started executing and this might take some time for to complete the execution but let's see okay again another let me just check it so I have figured out that my basically this video path was not correct so first of all this is small B so let me just correct it and I just need to remove this as well so now the video path is correct and I have just un done this unquented these lines okay so let's run it and see what results we get so basically our video path was not great so there were uh we were getting the error but I hope uh now it's fine and it will work fine but if there is an error we will definitely fix it up so this might take few more seconds uh before we get the results okay so it has started executing and it's looking to work fine but so now you can just let me aggregate my screen towards it okay so now you can see over here uh we are able to take the lessons plate and we have directed a license plate and here we have the confidence score so the next step what we want to do is we want to read this like that read the text on this lessons plate like here it is r183 JF so I want to read this text on the license plate we have detected our lessons plate I was but I want to read the text on this lessons played so I will just go back towards that code and just write the code for it and let's read the text on the lessons plate so to read the text under the sense plate I will be using easy or CR so first of all I will import easy or CR over here and then I will initialize this vocr so I will write reader is equal to easy or CR dot reader and we want to read the text in in the English language So currently the text is English language so I will write English and I don't have GPU in my local system so GPU will be false okay so next what I will do is over here is okay I will just create a function by the name OCR Dash image over here and the image in the input which we have for this function will be frame bx1 the y1 the X2 the Y2 coordinates so these are will be the inputs for this function or CR image and first to read the text on the license plate we just need to find uh the we just need to pass those coordinate coordinates of the morning box so here we just need to have X2 Y2 okay so first of all we will just want to have this those that area from where we want to read the lessons plate text so I will just write just write frame is equal to frame y1 0.2 X1 X2 so now we are only selecting the area where we have detected the license plate now we just need to convert the area where we have directed the license plate into grayscale so we just need to convert from BGR to grayscale so I will write gray is equal to CB2 dots pvt color and I will write frame comma dot color BGR to create so we are just converting that area from BGR to grayscale next I will write reader or I will write result is equal to reader so now here I have initialized the easier CR so I will write reader dot read text so now I have converted my license plate area from BGR to grayscale so now I am just reading that text in that area so I have initialized the ecos here over here so now using ezos here I'm just reading the test on the lessons plate and my text will be stored in this result variable so I will be showing you that print result okay and okay so just leave it over here and that's uh go over here and just initialize that is used here from here so now what I will do is first of all just comment out this and just press enter over here so now I will be calling this function below so I will write text is equal to just copy the name of this function from here and write text to OCR image and I will pass frame X1 y1 X2 and Y2 coordinates and let me just print this text over here okay so I'm just trying to print this result and then I will show you how we can read the text and just display on the license plate the text so let me just run this and see what results do we get so I'm just executing the script and hopefully it will run but if there is any error again we will try to fix it up so it's executing but it will take some time uh to complete the execution process so let it complete the execution and then we will go ahead so it is going okay so okay okay so I have just commented out the levels of yeah you can expect that error okay okay so I don't want to just show the output currently okay so let me just run let's see uh hopefully it will operate the results I forgot to comment these lines uh so I have commented these lines now and let's see what results do we get now so this might take few more seconds uh before we were able to see the results over here okay [Music] okay so this is for the first frame and this is for the second frame okay let's go for a five and then I will just pause this video and let's see so here no license plate is detected okay at the start okay okay okay I got it I got it okay so basically we are just printing this text and here you can see this text over here you can see this text over here so now what we can see that uh here we have a list and inside this list we have this complete list over here and here you can see that these are the coordinates of the bounding box like we have drawn a bounding box or a rectangle around the lessons plate so these are the coordinate for this rectangle and here we have The Confidence Code like this 0.10 means the model is 10 confident that this is the lessons paid and here 0.30 means the model is 30 confident that this is the lessons plate and here is the lessons plate text so this is the text of the lessons played so now we just want to display this text above the bounding box okay so here is the text of the essence plate and inside this list we have this complete in the bracket so we have only one term inside this list okay so you can see over here I have created a OCR image so now I will just add few things and we will be able to display the text on the lesson split so what I will do over here let me just com I can no need to find the site text is equal to okay and then press enter over here now I will Loop through each of the result for result rest in result and I will just write over here if length result [Music] is equal to 1. So currently here you can see the length is equal to 1 because inside this list we have the brackets and this is completely one term one thing okay if length resist result is equal to 1 text is equal to press one Okay so okay so now inside this list this is the one term and inside that this is the other term and then in the one part we have this okay and further I will write if length result is greater than 1 and length res 1 is greater than 6 and rest to means the confidence is greater than 0.2 then so in that case we all will also print the text over here Z1 so that's fine and leave the print now return spring text so that's fine completely now Okay so what next we need to do just display this text on the bounding box and I will also save the output video okay let's see if it works uh okay I just want to change the color of the bonding box and the rectangle so just give me a minute let me just look for some other color schemes I might have saved in my system okay so just give me a minute let me look for these kind of schemes so now I've just made the change in the color schemes over here and just added few different colors because it was previously appearing up maroon and it was not looking very good so I've just updated the color schemes and let's see how it goes so this might take few more seconds and before we are able to see the results over here thank you okay so this might take few more seconds before we have the results or the output in front of us okay so here we get the label error and it is in line 60. label okay so that was my mistake I'm just not returned label is equal to voice label is equal to text so hopefully it will not show the output over here and let's see I think that error is now fixed and we will have the output now so it hasn't started executing and let's see what output do we get so this might take few more seconds so you can see here I am using CPU this module is much faster with a GPU you can see this line over here so if you have GPU in your system that is best case so let me just so now you can see over here we have detected the license plate okay let me just navigate my screen sorry I again forgot to navigate Okay so so now you can see over here now you can see the result we have detected the license plate plus I am also displaying the text on this result result plate like r183 JF so we have detected a license plate plus we are also able to display the text on this lessons played so that's amazing uh in this way we will detect and read the text on the other lessons plates as well so currently I am using CPU so the detection is very slow but if you just follow this format you will be able to detect the license plate plus you will also be able to read the text on this lessons played as well as we are doing over here [Music] we are able to read the text on this lessons late as well so in this way we will be able to read the text on the other lessons plate of different pickles as well so that's all from this video tutorial I hope you have learned something from this video tutorial see you all in the next video till then bye bye\", metadata={'source': 'aJHBDCFAZ-g', 'title': 'Automatic Number Plate Recognition using YOLO-NAS and EasyOCR  ( Images & Videos)', 'description': 'Unknown', 'view_count': 3387, 'thumbnail_url': 'https://i.ytimg.com/vi/aJHBDCFAZ-g/hq720.jpg', 'publish_date': '2023-06-12 00:00:00', 'length': 2367, 'author': 'Muhammad Moin'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 07: YouTube Video Summary**"
      ],
      "metadata": {
        "id": "mSIeh9hW1C7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm=OpenAI(temperature=0.6)"
      ],
      "metadata": {
        "id": "nuUbTCLq07Ua"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "1B8bfdiB1eci"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)"
      ],
      "metadata": {
        "id": "6tPixoOO1ofH"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = text_splitter.split_documents(result)"
      ],
      "metadata": {
        "id": "CgCTtRwe12Eq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT4lSbRp2BWi",
        "outputId": "aec113d0-51af-4a5c-80b6-36097cf7549c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8rlsCLo1714",
        "outputId": "170215d2-218f-46b1-84df-e73136dc74d4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\"hello everyone in this video tutorial we will see how we can do license plate detection and recognition using YOLO Nas so here is the complete notebook script I will be guiding you step by step on how you can do license plate detection and recognition using yolanas for license plate detection I will be using your own asmo object detection order and to read the text from the lessons plate I will be using easy OCR so to detect a license plate I will use YOLO Nas model and to read the text on the lessons plate I will be using easy OCR so let's get started so here you can see that we have the license plate data set over here the data set consists of 300 images and here we have the air ratio like we have 601 images in the training set 64 images in the validation set and 30 images in the test set and the splitting ratio is eighty percent six percent of the images are in a training set nine percent of the images are in the validation set and four percent of the images are in that testing set\", metadata={'source': 'aJHBDCFAZ-g', 'title': 'Automatic Number Plate Recognition using YOLO-NAS and EasyOCR  ( Images & Videos)', 'description': 'Unknown', 'view_count': 3387, 'thumbnail_url': 'https://i.ytimg.com/vi/aJHBDCFAZ-g/hq720.jpg', 'publish_date': '2023-06-12 00:00:00', 'length': 2367, 'author': 'Muhammad Moin'})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yBPgwYf177l",
        "outputId": "a95f39ea-cde3-480e-a910-24505b4ecfbd"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\"and the splitting ratio is eighty percent six percent of the images are in a training set nine percent of the images are in the validation set and four percent of the images are in that testing set and if you just see over here we have only one class in this data set which is of lessons and here you can see the images so you can see that we have only one class in our data set which is of lessons we don't have multi-classes and here you can see that a bounding box uh is being created around the lessons plate in the images like you can see over here uh bounding boxes are created around the lessons plate in the images like you can see over here you can see over here as well and you can see over here as well like I like bounding boxes have been created around the license plates if I go to the next image you can see that we have the bounding box around the lessons plate so I will be training or fine tuning my YOLO YOLO Nas model on this license plate data set so after training the Uranus\", metadata={'source': 'aJHBDCFAZ-g', 'title': 'Automatic Number Plate Recognition using YOLO-NAS and EasyOCR  ( Images & Videos)', 'description': 'Unknown', 'view_count': 3387, 'thumbnail_url': 'https://i.ytimg.com/vi/aJHBDCFAZ-g/hq720.jpg', 'publish_date': '2023-06-12 00:00:00', 'length': 2367, 'author': 'Muhammad Moin'})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(llm=llm, chain_type = 'map_reduce', verbose=False)"
      ],
      "metadata": {
        "id": "Az4P8pSD1E9m"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "2B-sQSYZ1KBx",
        "outputId": "5dca1a26-822e-4780-be8f-95ef779ed945"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' This text outlines the process of training a YOLO Nas model on a license plate data set in order to detect the license plate in images and videos. After detecting the license plate, Easy OCR is used to read the text on it. The code is provided and the reader should make sure to select the GPU before running the script. The results are seen to be \"fine\" and the model works \"very profine.\" The output video is saved by the name detections.mp4 and the author has done the detection. The Mean Average Precision (MAP) was 90.88 and the F1 score with IOU 0.5 was 0.1017. The system was then tested on a sample image to determine if the license plate could be detected, which it was, confirming the success of the system.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(llm=llm, chain_type = 'map_reduce', verbose=True)"
      ],
      "metadata": {
        "id": "IE7_82cG1LfC"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(texts[:4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q8vHveAj3GX2",
        "outputId": "6dfbd90e-2fe3-4595-874c-cadb47b9fd2d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"hello everyone in this video tutorial we will see how we can do license plate detection and recognition using YOLO Nas so here is the complete notebook script I will be guiding you step by step on how you can do license plate detection and recognition using yolanas for license plate detection I will be using your own asmo object detection order and to read the text from the lessons plate I will be using easy OCR so to detect a license plate I will use YOLO Nas model and to read the text on the lessons plate I will be using easy OCR so let's get started so here you can see that we have the license plate data set over here the data set consists of 300 images and here we have the air ratio like we have 601 images in the training set 64 images in the validation set and 30 images in the test set and the splitting ratio is eighty percent six percent of the images are in a training set nine percent of the images are in the validation set and four percent of the images are in that testing set\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"and the splitting ratio is eighty percent six percent of the images are in a training set nine percent of the images are in the validation set and four percent of the images are in that testing set and if you just see over here we have only one class in this data set which is of lessons and here you can see the images so you can see that we have only one class in our data set which is of lessons we don't have multi-classes and here you can see that a bounding box uh is being created around the lessons plate in the images like you can see over here uh bounding boxes are created around the lessons plate in the images like you can see over here you can see over here as well and you can see over here as well like I like bounding boxes have been created around the license plates if I go to the next image you can see that we have the bounding box around the lessons plate so I will be training or fine tuning my YOLO YOLO Nas model on this license plate data set so after training the Uranus\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"next image you can see that we have the bounding box around the lessons plate so I will be training or fine tuning my YOLO YOLO Nas model on this license plate data set so after training the Uranus model on this lessons play data set we will be testing our model on images and videos and we will be seeing whether our model is able to detect a license plate or not so if our model is able to detect the license plate then we will read the text on the lessons plate using easy or CR okay so you can see over here I will just use easy or CR to read the text on the lessons plate like here we have mh20bq20 so first of all I will be training or fine tuning the yolona small model on this lessons play data set so that we can detect the lesson split and after detecting that license plate I will use Easy OCR to read the text on the lessons plate okay so here is the code so before running the script uh please make sure that you have selected the GPU so I will just uh click on runtime and click on\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"Easy OCR to read the text on the lessons plate okay so here is the code so before running the script uh please make sure that you have selected the GPU so I will just uh click on runtime and click on change runtime and you can see over here I have selected the hardware accelerator as GPU and GPU type is T4 okay so in the first step you need to install the packages so I have already run this cell and installed the packages because it's a Time ticking and take around 10 to 11 minutes to install the packages so after installing the packages we need to restart the runtime so I will just click on hit restart runtime over here and just restart the runs time then in the next step I will import all the required libraries so I will just run this cell and this cell and this cell as well so now uh after importing all the required libraries in the steps three I need to set the checkpoint directory and just write the and create a subfolder inside the checkpoint directory where I will just save my\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\" In this video tutorial, we will learn how to do license plate detection and recognition using YOLO Nas. The data set consists of 300 images, with 80% in the training set, 6% in the validation set, and 4% in the test set. We will be using YOLO Nas for license plate detection and Easy OCR to read the text from the license plate.\n",
            "\n",
            "\n",
            "This article discusses a data set that consists of one class, license plates, with a splitting ratio of 80/6/9/4 for training, validation, and testing sets respectively. Bounding boxes are created around the license plates in the images. The YOLO Nas model will be trained and fine tuned on the license plate data set.\n",
            "\n",
            "\n",
            "This code explains how to train a YOLO YOLO Nas model on a license plate dataset, and then use Easy OCR to read the text on the license plates when the model is able to detect them. It also explains how to select the GPU before running the script.\n",
            "\n",
            " This text details the code to install and set up a GPU for OCR reading. It includes steps to install packages, restart the runtime, import libraries, and create a checkpoint directory with a subfolder.\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' This article explains how to use YOLO Nas to detect license plates and Easy OCR to read the text from the license plates in a dataset with a splitting ratio of 80/6/4 for training, validation, and testing sets respectively. It also explains how to install and set up a GPU for OCR reading.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 08: Multiple YouTube Videos | Generate a Summary**"
      ],
      "metadata": {
        "id": "P-YivsX53d2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "youtube_url_list = [\"https://www.youtube.com/watch?v=Ijk6R5EHP7s\", \"https://www.youtube.com/watch?v=aJHBDCFAZ-g\"]"
      ],
      "metadata": {
        "id": "hEZl3mdo3IvP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = []\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap = 200)\n",
        "\n",
        "\n",
        "for url in youtube_url_list:\n",
        "  loader = YoutubeLoader.from_youtube_url(url, add_video_info=True)\n",
        "  result=loader.load()\n",
        "  text.extend(text_splitter.split_documents(result))"
      ],
      "metadata": {
        "id": "cVR0bHFe3k3Z"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQcrn7M24RX1",
        "outputId": "26f4164d-62f2-4710-f13d-6dbe7c5880cd"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.6)"
      ],
      "metadata": {
        "id": "QN961SEd4bri"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(llm=llm, chain_type = 'map_reduce', verbose=False)"
      ],
      "metadata": {
        "id": "NIxHI5rF4gGo"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "WVXWUqQL4iwJ",
        "outputId": "72b0a889-8deb-4804-ae5f-b5e8bbd83709"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' This video tutorial explains how to detect and read the text on license plates using a YOLO Nas model and Easy OCR. The model is trained on a license plate dataset, and then tested on images and videos. The results are outputted as a video, and the script is run to check for errors. The color of the bounding box is changed, and the text is displayed. The model is also tested on car crashes and found to be successful.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O_VzS6a14loN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}